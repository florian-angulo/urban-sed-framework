{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346a2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import os\n",
    "from nnet import CRNN\n",
    "from pl_trainer import SEDTask4_2021\n",
    "from utils.encoder import ManyHotEncoder\n",
    "from utils.utils import batched_decode_preds\n",
    "import scipy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "import librosa\n",
    "import librosa.display\n",
    "import time\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import h5py\n",
    "from IPython.display import Audio\n",
    "from processing.sampler import ConcatDatasetBatchSampler\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing.datasets import ConcatDatasetUrban, HDF5_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d20c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openl3\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c673ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hear21passt.base2levelmel import load_model, get_scene_embeddings, get_timestamp_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce692f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config/sed_hear.yaml\", \"r\") as f:\n",
    "        conf = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe2f92",
   "metadata": {},
   "source": [
    "###  PASST model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633ed1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_passt = load_model()\n",
    "model_passt = model_passt.cuda()\n",
    "model_passt.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbb6be",
   "metadata": {},
   "source": [
    "### torchopenl3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampler = torchaudio.transforms.Resample(32000, 48000).cuda()\n",
    "model_openl3 = torchopenl3.models.load_audio_embedding_model(input_repr=\"mel256\", content_type=\"env\",\n",
    "                                                 embedding_size=6144).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c805fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embed, time_stamps = get_timestamp_embeddings(audio, model_passt)\n",
    "#embed = get_scene_embeddings(audio, model_passt)\n",
    "print(embed.shape)\n",
    "print(time_stamps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb, ts = torchopenl3.get_audio_embedding(audio_res, sr=48000, model=model, hop_size=0.0615)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f19b43",
   "metadata": {},
   "source": [
    "### Computing embeddings in HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f50f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.close()\n",
    "hf_audio.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af1ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type_hf = \"train\"\n",
    "\n",
    "hf_audio = h5py.File(conf[\"data\"][\"root_path\"] + \"train.h5\", 'r+')\n",
    "hf = h5py.File(conf[\"data\"][\"root_path\"] + conf[\"data\"][f\"hdf5_{type_hf}\"], 'r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_hf = \"train\"\n",
    "dset = \"SINGA-PURA\"\n",
    "hf = h5py.File(conf[\"data\"][\"root_path\"] + conf[\"data\"][f\"hdf5_{type_hf}\"], 'r')\n",
    "embeds = hf[dset][\"open_l3_512\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bac16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(embeds.shape[0])):\n",
    "    print(np.sum(embeds[i]))\n",
    "    assert np.sum(embeds[i]) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0084e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show = \"SINGA-PURA\"\n",
    "for d in hf[to_show]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09eec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = \"SINGA-PURA\"\n",
    "audio = hf_audio[dset][\"audio_32k\"] if type_hf == \"train\" else hf[dset][\"audio_32k\"]\n",
    "group = hf[dset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512\n",
    "model_openl3 = openl3.models.load_audio_embedding_model(input_repr=\"mel256\", content_type=\"env\",embedding_size=embed_size, frontend='kapre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34acf9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb, ts = openl3.get_audio_embedding(audio[0], sr=32000, model=model_openl3, hop_size=0.0615, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbba825",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 156\n",
    "name = \"open_l3_\" + str(embed_size)\n",
    "print(\"embed_size :\", embed_size)\n",
    "print(\"n_frames :\", n_frames)\n",
    "print(\"name :\", name)\n",
    "print(\"group :\", group)\n",
    "sure = input(f\"are you sure ? (y/N)\")\n",
    "if sure == \"y\":\n",
    "    if name not in group:\n",
    "        group.create_dataset(name, (audio.shape[0], n_frames, embed_size,))\n",
    "    embeddings_arr = group[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c425f",
   "metadata": {},
   "source": [
    "### openl3 computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(checkpoint, audio.shape[0])):\n",
    "    emb, ts = openl3.get_audio_embedding(audio[i], sr=32000, model=model_openl3, hop_size=0.0615, verbose=False)\n",
    "    embeddings_arr[i] = emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffee0ec",
   "metadata": {},
   "source": [
    "### Torchopenl3 computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "assert audio.shape[0] % batch_size\n",
    "for k in tqdm(range(audio.shape[0]//batch_size+1)):\n",
    "    audio_batch = torch.from_numpy(audio[k*batch_size:(k+1)*batch_size]).cuda()\n",
    "    with torch.no_grad():\n",
    "        audio_res = resampler(audio_batch).cuda()\n",
    "        emb_openl3, ts_openl3 = torchopenl3.get_audio_embedding(audio_res, sr=48000, model=model_openl3, hop_size=0.0615)\n",
    "    emb_cpu = emb_openl3.cpu().numpy()\n",
    "    embeddings_arr[k*batch_size:(k+1)*batch_size] = emb_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c94a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "assert audio.shape[0] % batch_size\n",
    "for k in tqdm(range(audio.shape[0]//batch_size+1)):\n",
    "    audio_batch = torch.from_numpy(audio[k*batch_size:(k+1)*batch_size]).cuda()\n",
    "    with torch.no_grad():\n",
    "        emb_openl3, ts_openl3 = torchopenl3.get_audio_embedding(audio_res, sr=32000, model=model_openl3, hop_size=0.0615)\n",
    "    emb_cpu = emb_openl3.cpu().numpy()\n",
    "    embeddings_arr[k*batch_size:(k+1)*batch_size] = emb_cpu\n",
    "group_sgp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e68256",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad3f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_res = resampler(audio_batch).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b23200",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_openl3, ts_openl3 = torchopenl3.get_audio_embedding(audio_res, sr=48000, model=model_openl3, hop_size=0.0615)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16da9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_openl3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ac415",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.decode_strong(hf[\"SINGA-PURA\"][\"groundtruth\"][taxo_name][3].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5361d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = hf[\"SINGA-PURA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = SONYC_test_set.ids\n",
    "print(len(ids))\n",
    "print(len(group[\"audio_32k\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5258a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ids) == len(group[\"audio_32k\"]):\n",
    "    print(\"ouais\")\n",
    "    group.create_dataset(\"filenames\", data=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_test[\"filenames_sg\"][0:10].astype(str)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd63de1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in hf[\"SONYC\"][\"groundtruth\"]:\n",
    "    print(hf[\"SONYC\"][\"groundtruth\"][f][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f12dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxo_name = \"taxonomy_coarse_SONYC\"\n",
    "with open(f\"../config/{taxo_name}.yaml\", \"r\") as f:\n",
    "        taxonomy = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b064638",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ManyHotEncoder(\n",
    "        taxonomy,\n",
    "        audio_len=conf[\"data\"][\"audio_max_len\"],\n",
    "        frame_len=conf[\"features\"][\"n_filters\"],\n",
    "        frame_hop=conf[\"features\"][\"hop_length\"],\n",
    "        net_pooling=conf[\"data\"][\"net_subsample\"],\n",
    "        fs=conf[\"data\"][\"fs\"],\n",
    "    )\n",
    "\n",
    "dataset = HDF5_SONYC_Dataset(\n",
    "    conf[\"data\"][\"root_path\"] + conf[\"data\"][f\"hdf5_{type_hf}\"],\n",
    "    conf[\"data\"][\"root_path\"] + conf[\"data\"][f\"sonyc_csv_{type_hf}\"],\n",
    "    encoder,\n",
    "    return_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b78a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(taxonomy[\"class_labels\"]))\n",
    "print(set(taxonomy[\"SONYC\"].values()))\n",
    "print(set(taxonomy[\"SINGA-PURA\"].values()))\n",
    "print(len(taxonomy[\"class_labels\"]))\n",
    "print(len(set(taxonomy[\"SONYC\"].values())))\n",
    "print(len(set(taxonomy[\"SINGA-PURA\"].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a11861",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros((len(dataset), len(taxonomy[\"class_labels\"]),))\n",
    "i = 0\n",
    "for batch in dataset:\n",
    "    labels[i] = batch[1]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ada97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "print(batch[1])\n",
    "print(encoder.decode_weak(batch[1]))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329143a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = hf[\"SONYC\"]\n",
    "group.create_dataset(f\"groundtruth/{taxo_name}\", data=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348a9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf[f\"SONYC\"][\"groundtruth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGAPURA_train_set.ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGAPURA_train_set.ids[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55bb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for batch in SINGAPURA_train_set:\n",
    "    assert batch[1].shape == (8, 156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f521c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.zeros((8,9))\n",
    "test = test.transpose()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ab3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGAPURA_val_set = HDF5_SINGAPURA_labelled(\n",
    "    conf[\"data\"][\"root_path\"] + conf[\"data\"][\"hdf5_val\"],\n",
    "    conf[\"data\"][\"root_path\"] + conf[\"data\"][\"singa-pura_csv_val\"],\n",
    "    encoder,\n",
    "    return_filename=True,\n",
    "    taxonomy=taxonomy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f03ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SONYC_train_set = HDF5_SONYC_Dataset(\n",
    "    conf[\"data\"][\"root_path\"] + conf[\"data\"][\"hdf5_train\"],\n",
    "    conf[\"data\"][\"root_path\"] + conf[\"data\"][\"sonyc_csv_train\"],\n",
    "    encoder,\n",
    "    return_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5fafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SONYC_val_set = HDF5_SONYC_Dataset(\n",
    "    conf[\"data\"][\"root_path\"] + conf[\"data\"][\"hdf5_val\"],\n",
    "    conf[\"data\"][\"root_path\"] + conf[\"data\"][\"sonyc_csv_val\"],\n",
    "    encoder,\n",
    "    return_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db260e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGAPURA_test_set = HDF5_SINGAPURA_labelled(\n",
    "    conf[\"data\"][\"root_path\"] + conf[\"data\"][\"hdf5_test\"],\n",
    "    conf[\"data\"][\"root_path\"] + conf[\"data\"][\"singa-pura_csv_test\"],\n",
    "    encoder,\n",
    "    return_filename=True,\n",
    ")\n",
    "\n",
    "SONYC_test_set = HDF5_SONYC_Dataset(\n",
    "    conf[\"data\"][\"root_path\"] + conf[\"data\"][\"hdf5_test\"],\n",
    "    conf[\"data\"][\"root_path\"] + conf[\"data\"][\"sonyc_csv_test\"],\n",
    "    encoder,\n",
    "    return_filename=True,\n",
    ")\n",
    "test_dataset = torch.utils.data.ConcatDataset([SINGAPURA_test_set, SONYC_test_set])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d573d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.datasets[0] == SINGAPURA_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d5efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations, groundtruths = SINGAPURA_val_set._generate_eval_dfs(taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f331f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGAPURA_val_set.groundtruths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db0405",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sonyc_train = pd.read_csv(\"/tsi/doctorants/fangulo/Detection-Urban/data/metadata/train/SONYC_train.csv\")\n",
    "df_sonyc_val = pd.read_csv(\"/tsi/doctorants/fangulo/Detection-Urban/data/metadata/val/SONYC_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa18b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = soundata.initialize(\"singapura\", \"/tsi/dcase/SINGA-PURA\")\n",
    "ids = dset.clip_ids  # the list of urbansound8k's clip ids\n",
    "clips = dset.load_clips()  # Load all clips in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71620e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clips[SINGAPURA_train_set.ids[1]].events.annotations[0].labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGAPURA_train_set[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24507259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(400)\n",
    "audio, labels, filename = SONYC_train_set[idx]\n",
    "print(idx)\n",
    "print(labels.shape)\n",
    "print(labels[:, 0])\n",
    "print(np.where(labels))\n",
    "print(filename)\n",
    "print(df_sonyc_train[df_sonyc_train[\"filename\"]==filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b83a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dataset = np.zeros((300,8))\n",
    "labels[:,0]\n",
    "labels_dataset[0]=labels[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4941d69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for audio, labels, filename in SONYC_train_set:\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3583180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(400)\n",
    "audio, labels, filename = SINGAPURA_train_set[idx]\n",
    "print(idx)\n",
    "print(labels.shape)\n",
    "print(filename)\n",
    "print(clips[filename].events.annotations[0].labels)\n",
    "print(clips[filename].events.annotations[0].intervals[:,0])\n",
    "print(clips[filename].events.annotations[0].intervals[:,1])\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadb438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRNN.CRNN(**conf['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e596f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab82c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = torch.ones((24, 128, 626))\n",
    "print(model(input_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337167d3",
   "metadata": {},
   "source": [
    "### Computing HDF5 SONYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8846f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/Urban Sound Exploration/datasets_conf.yaml\", \"r\") as f:\n",
    "        dconf = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f3bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_split(annotation_data):\n",
    "    \"\"\"\n",
    "    Get indices for train and validation subsets\n",
    "    Parameters\n",
    "    ----------\n",
    "    annotation_data\n",
    "    Returns\n",
    "    -------\n",
    "    train_idxs\n",
    "    valid_idxs\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the audio filenames and the splits without duplicates\n",
    "    data = annotation_data[['split', 'audio_filename', 'annotator_id']]\\\n",
    "                          .groupby(by=['split', 'audio_filename'], as_index=False)\\\n",
    "                          .min()\\\n",
    "                          .sort_values('audio_filename')\n",
    "\n",
    "    train_idxs = []\n",
    "    valid_idxs = []\n",
    "    test_idxs = []\n",
    "\n",
    "    for idx, (_, row) in enumerate(data.iterrows()):\n",
    "        if row['split'] == 'train':\n",
    "            train_idxs.append(idx)\n",
    "        elif row['split'] == 'validate' and row['annotator_id'] <= 0:\n",
    "            # For validation examples, only use verified annotations\n",
    "            valid_idxs.append(idx)\n",
    "        elif row['split'] == 'test' and row['annotator_id'] <= 0:\n",
    "            # For validation examples, only use verified annotations\n",
    "            test_idxs.append(idx)\n",
    "\n",
    "    return np.array(train_idxs), np.array(valid_idxs), np.array(test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_sonyc = pd.read_csv(dconf['sonyc']['annotations']).sort_values('audio_filename')\n",
    "subset_split = get_subset_split(annot_sonyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba48f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtale.show(annot_sonyc.iloc[subset_split[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba5aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = annot_sonyc[['split', 'audio_filename', 'annotator_id']]\\\n",
    "                          .groupby(by=['split', 'audio_filename'], as_index=False)\\\n",
    "                          .min()\\\n",
    "                          .sort_values('audio_filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7028cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c014c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs = []\n",
    "valid_idxs = []\n",
    "test_idxs = []\n",
    "\n",
    "for idx, (_, row) in enumerate(data.iterrows()):\n",
    "    print(idx)\n",
    "    if row['split'] == 'train':\n",
    "        train_idxs.append(idx)\n",
    "    elif row['split'] == 'validate' and row['annotator_id'] <= 0:\n",
    "        # For validation examples, only use verified annotations\n",
    "        valid_idxs.append(idx)\n",
    "    elif row['split'] == 'test' and row['annotator_id'] <= 0:\n",
    "        # For validation examples, only use verified annotations\n",
    "        test_idxs.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11795a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ground_truth(annotation_path, yaml_path, split=\"validate\", type_annot=\"verified\"):\n",
    "    \"\"\"\n",
    "    Parse ground truth annotations from a CSV file containing both fine-level\n",
    "    and coarse-level predictions (and possibly additional metadata).\n",
    "    Returns a Pandas DataFrame in which the column names are coarse\n",
    "    IDs of the form 1, 2, 3 etc.\n",
    "    Parameters\n",
    "    ----------\n",
    "    annotation_path: string\n",
    "        Path to the CSV file containing predictions.\n",
    "    yaml_path: string\n",
    "        Path to the YAML file containing coarse taxonomy.\n",
    "    Returns\n",
    "    -------\n",
    "    gt_df: DataFrame\n",
    "        Ground truth.\n",
    "    \"\"\"\n",
    "    # Create dictionary to parse tags\n",
    "    with open(yaml_path, 'r') as stream:\n",
    "        yaml_dict = yaml.load(stream, Loader=yaml.Loader)\n",
    "\n",
    "    # Load CSV file into a Pandas DataFrame.\n",
    "    ann_df = pd.read_csv(annotation_path)\n",
    "    \n",
    "    if type_annot == \"verified\":\n",
    "        # Restrict to ground truth (\"annotator zero\").\n",
    "        gt_df = ann_df[\n",
    "            (ann_df[\"annotator_id\"]==0) & (ann_df[\"split\"]==split)]\n",
    "    elif type_annot == \"crowdsourced\":\n",
    "        gt_df = ann_df[\n",
    "            (ann_df[\"annotator_id\"] != 0) & (ann_df[\"split\"]==split)]\n",
    "    elif type_annot == \"all\":\n",
    "        gt_df = ann_df[\n",
    "            ann_df[\"split\"]==split]\n",
    "    \n",
    "    # Rename coarse columns.\n",
    "    coarse_dict = yaml_dict[\"coarse\"]\n",
    "    coarse_renaming = {\n",
    "        \"_\".join([str(c), coarse_dict[c], \"presence\"]): str(c)\n",
    "        for c in coarse_dict}\n",
    "    gt_df = gt_df.rename(columns=coarse_renaming)\n",
    "\n",
    "    # Collect tag names as strings and map them to mixed (coarse-fine) ID pairs.\n",
    "    # The \"mixed key\" is a hyphenation of the coarse ID and fine ID.\n",
    "    fine_dict = {}\n",
    "    for coarse_id in yaml_dict[\"fine\"]:\n",
    "        for fine_id in yaml_dict[\"fine\"][coarse_id]:\n",
    "            mixed_key = \"-\".join([str(coarse_id), str(fine_id)])\n",
    "            fine_dict[mixed_key] = yaml_dict[\"fine\"][coarse_id][fine_id]\n",
    "\n",
    "    # Rename fine columns.\n",
    "    fine_renaming = {\"_\".join([k, fine_dict[k], \"presence\"]): k\n",
    "        for k in fine_dict}\n",
    "    gt_df = gt_df.rename(columns=fine_renaming)\n",
    "\n",
    "    # Loop over coarse tags.\n",
    "    n_samples = len(gt_df)\n",
    "    coarse_dict = yaml_dict[\"coarse\"]\n",
    "    for coarse_id in yaml_dict[\"coarse\"]:\n",
    "        # Construct incomplete fine tag by appending -X to the coarse tag.\n",
    "        incomplete_tag = str(coarse_id) + \"-X\"\n",
    "\n",
    "        # If the incomplete tag is not in the prediction, append a column of zeros.\n",
    "        # This is the case e.g. for coarse ID 7 (\"dogs\") which has a single\n",
    "        # fine-level tag (\"7-1_dog-barking-whining\") and thus no incomplete\n",
    "        # tag 7-X.\n",
    "        if incomplete_tag not in gt_df.columns:\n",
    "            gt_df[incomplete_tag] = np.zeros((n_samples,)).astype('int')\n",
    "\n",
    "    # Return output in DataFrame format.\n",
    "    return gt_df.sort_values('audio_filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtale.show(parse_ground_truth(dconf['sonyc']['annotations'], dconf['sonyc']['taxonomy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677204f0",
   "metadata": {},
   "source": [
    "### Transforming dataframes into simpler ones \n",
    "Audio tagging : (filename, tags) <br>\n",
    "SED : filename, (onset, offset, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3391aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = parse_ground_truth(dconf['sonyc']['annotations'], dconf['sonyc']['taxonomy'], split=\"test\", type_annot=\"verified\")\n",
    "dtale.show(gt_df)\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81124d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = list(gt_df.columns)\n",
    "print(labels)\n",
    "print(labels[12:41] + labels[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(gt_df.columns)\n",
    "labels = columns[12:41] + columns[-10:]\n",
    "weak_dict_set = defaultdict(set)\n",
    "\n",
    "for _, row in gt_df.iterrows():\n",
    "    for l in labels:\n",
    "        if row[l]:\n",
    "            weak_dict_set[row['audio_filename']].add(l)\n",
    "\n",
    "weak_dict = {k:list(v) for k,v in weak_dict_set.items()}\n",
    "for fname in gt_df[\"audio_filename\"].unique():\n",
    "    if fname not in weak_dict.keys():\n",
    "        weak_dict[fname] = '0'\n",
    "\n",
    "weak_dict_str = {k: ';'.join(v) for k,v in weak_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f3f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weak = pd.DataFrame.from_dict({'filename':weak_dict_str.keys(), 'event_labels':weak_dict_str.values()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_weak.to_csv('../data/metadata/test/SONYC_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86935de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for idx in ids:\n",
    "    for l in clips[idx].events.annotations[0].labels:\n",
    "        if l not in labels:\n",
    "            labels.append(l)\n",
    "labels.sort()\n",
    "print(labels)\n",
    "#    if '0-3' in clips[idx].events.annotations[0].labels:\n",
    "#        raise ValueError(f\"FilenameÂ {idx} has wrong label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clips[\"[b827ebd63759][2020-10-28T00-30-24Z][manual][---][4edbade2d41d5f80e324ee4f10d401c0]\"].events.annotations[0].labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92029264",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_clip = clips[ids[0]]  # Get the first clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clips[ids[0]].clip_id == ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7651b954",
   "metadata": {},
   "source": [
    "### Transforming SINGA-PURA dataframes into train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sgp = pd.read_csv(conf['singa-pura']['annotations_recording'])\n",
    "sensor_ids = ['b827eb0ebf2f', 'b827eb7680c5','b827eb3e52b8']\n",
    "df_sgp_train = df_sgp[(~df_sgp[\"sensor_id\"].isin(sensor_ids)) & ((df_sgp[\"month\"] != 8) | (df_sgp[\"date\"] > 21))].reset_index()\n",
    "df_sgp_val = df_sgp[(df_sgp[\"sensor_id\"].isin(sensor_ids)) & ((df_sgp[\"month\"] != 8) | (df_sgp[\"date\"] > 21))].reset_index()\n",
    "df_sgp_test = df_sgp[(df_sgp[\"date\"] <= 21) & (df_sgp[\"month\"]==8)].reset_index()\n",
    "print(len(df_sgp_train), len(df_sgp_test), len(df_sgp_val))\n",
    "len(df_sgp_train)+len(df_sgp_test)+len(df_sgp_val)\n",
    "\n",
    "df_sgp_train[\"clip_id\"] = df_sgp_train[\"filename\"].apply(lambda x : x.replace(\".flac\", \"\"))\n",
    "df_sgp_val[\"clip_id\"] = df_sgp_val[\"filename\"].apply(lambda x : x.replace(\".flac\", \"\"))\n",
    "df_sgp_test[\"clip_id\"] = df_sgp_test[\"filename\"].apply(lambda x : x.replace(\".flac\", \"\"))\n",
    "\n",
    "df_sgp_train[\"filename\"] = df_sgp_train[\"foldername\"] + \"/\" + df_sgp_train[\"filename\"]\n",
    "df_sgp_val[\"filename\"] = df_sgp_val[\"foldername\"] + \"/\" + df_sgp_val[\"filename\"]\n",
    "df_sgp_test[\"filename\"] = df_sgp_test[\"foldername\"] + \"/\" + df_sgp_test[\"filename\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtale.show(df_sgp_train[[\"filename\",\"clip_id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4374e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sgp_train[[\"filename\",\"clip_id\"]].to_csv(\"../data/metadata/train/SINGA-PURA_train.csv\", index=False)\n",
    "df_sgp_val[[\"filename\",\"clip_id\"]].to_csv(\"../data/metadata/val/SINGA-PURA_val.csv\", index=False)\n",
    "df_sgp_test[[\"filename\",\"clip_id\"]].to_csv(\"../data/metadata/test/SINGA-PURA_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af82a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0,500)\n",
    "onset = clips[df_sgp_train[\"clip_id\"][idx]].events.annotations[0].intervals[:,0]\n",
    "offset = clips[df_sgp_train[\"clip_id\"][idx]].events.annotations[0].intervals[:,1]\n",
    "label = clips[df_sgp_train[\"clip_id\"][idx]].events.annotations[0].labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"onset\":onset, \"offset\":offset, \"label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sgp_train[[\"filename\",\"clip_id\"]].to_csv(\"/tsi/doctorants/fangulo/Detection-Urban/data/metadata/train/SINGA-PURA_train.csv\", index=False)\n",
    "df_sgp_val[[\"filename\",\"clip_id\"]].to_csv(\"/tsi/doctorants/fangulo/Detection-Urban/data/metadata/val/SINGA-PURA_val.csv\", index=False)\n",
    "df_sgp_test[[\"filename\",\"clip_id\"]].to_csv(\"/tsi/doctorants/fangulo/Detection-Urban/data/metadata/test/SINGA-PURA_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa78abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(\"../data/train.h5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(hf[\"SONYC\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec233c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(hf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9226e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(hf[\"SONYC\"][\"audio_32k\"][189], rate=32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab1a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sn = pd.read_csv(\"../data/metadata/train/SONYC_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sn.iloc[[189]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa7f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabelled = pd.read_csv(\"/tsi/dcase/SINGA-PURA/unlabelled_metadata_public_with_folder.csv\")\n",
    "del df_unlabelled[\"Unnamed: 0\"]\n",
    "df_unlabelled.to_csv(\"/tsi/dcase/SINGA-PURA/unlabelled_metadata_public_with_folder.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtale.show(df_unlabelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabelled[\"filename\"] = df_unlabelled[\"foldername\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d94c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabelled[\"filename\"] = df_unlabelled[\"foldername\"] +  \"/\" + df_unlabelled[\"filename\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c16246",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabelled[\"filename\"].to_csv(\"../data/metadata/train/SINGA-PURA_unlabelled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1fd514",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.create_dataset(\"SONYC/audio\", data=np.ones((41,50)))\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a46e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = h5py.File(\"test.h5\", \"r\")\n",
    "test[\"SONYC\"][\"melspec\"][10]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
